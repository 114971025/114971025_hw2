{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# A-1 TF-IDF文本相似度計算\n",
        "# ==================== 測試資料 ====================\n",
        "\n",
        "documents = [\n",
        "    \"人工智慧正在改變世界，機器學習是其核心技術\",\n",
        "    \"深度學習推動了人工智慧的發展，特別是在圖像識別領域\",\n",
        "    \"今天天氣很好，適合出去運動\",\n",
        "    \"機器學習和深度學習都是人工智慧的重要分支\",\n",
        "    \"運動有益健康，每天都應該保持運動習慣\"\n",
        "]"
      ],
      "metadata": {
        "id": "tLq-Fgx6GVbd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "# ==================== 第一部分：手動計算 TF-IDF ====================\n",
        "\n",
        "def calculate_tf(word_dict, total_words):\n",
        "    \"\"\"\n",
        "    計算詞頻\n",
        "    Args:\n",
        "        word_dict: 詞彙計數字典\n",
        "        total_words: 總詞數\n",
        "    Returns:\n",
        "        tf_dict: TF 值字典\n",
        "    \"\"\"\n",
        "    tf_dict = {}\n",
        "    for word, count in word_dict.items():\n",
        "        tf_dict[word] = count / total_words\n",
        "    return tf_dict\n",
        "\n",
        "\n",
        "def calculate_idf(documents, word):\n",
        "    \"\"\"\n",
        "    計算逆文件頻率\n",
        "    Args:\n",
        "        documents: 文件列表\n",
        "        word: 目標詞彙\n",
        "    Returns:\n",
        "        idf: IDF 值\n",
        "    \"\"\"\n",
        "    # 計算包含該詞的文件數\n",
        "    num_docs_containing_word = sum(1 for doc in documents if word in doc)\n",
        "\n",
        "    # 如果詞不存在於任何文件中，返回0\n",
        "    if num_docs_containing_word == 0:\n",
        "        return 0\n",
        "\n",
        "    # IDF = log(總文件數 / 包含該詞的文件數)\n",
        "    idf = math.log(len(documents) / num_docs_containing_word)\n",
        "    return idf\n",
        "\n",
        "\n",
        "def calculate_tfidf_manual(documents):\n",
        "    \"\"\"\n",
        "    手動計算所有文件的 TF-IDF\n",
        "    \"\"\"\n",
        "    tfidf_vectors = []\n",
        "\n",
        "    # 建立所有詞彙的集合\n",
        "    all_words = set()\n",
        "    for doc in documents:\n",
        "        all_words.update(doc.split())\n",
        "\n",
        "    # 對每個文件計算 TF-IDF\n",
        "    for doc in documents:\n",
        "        words = doc.split()\n",
        "        word_count = Counter(words)\n",
        "        total_words = len(words)\n",
        "\n",
        "        # 計算 TF\n",
        "        tf_dict = calculate_tf(word_count, total_words)\n",
        "\n",
        "        # 計算 TF-IDF\n",
        "        tfidf_dict = {}\n",
        "        for word in all_words:\n",
        "            if word in tf_dict:\n",
        "                tf = tf_dict[word]\n",
        "                idf = calculate_idf(documents, word)\n",
        "                tfidf_dict[word] = tf * idf\n",
        "            else:\n",
        "                tfidf_dict[word] = 0\n",
        "\n",
        "        tfidf_vectors.append(tfidf_dict)\n",
        "\n",
        "    return tfidf_vectors, sorted(all_words)\n",
        "\n",
        "\n",
        "def cosine_similarity_manual(vec1, vec2, all_words):\n",
        "    \"\"\"\n",
        "    手動計算餘弦相似度\n",
        "    \"\"\"\n",
        "    dot_product = sum(vec1[word] * vec2[word] for word in all_words)\n",
        "\n",
        "    magnitude1 = math.sqrt(sum(vec1[word] ** 2 for word in all_words))\n",
        "    magnitude2 = math.sqrt(sum(vec2[word] ** 2 for word in all_words))\n",
        "\n",
        "    if magnitude1 == 0 or magnitude2 == 0:\n",
        "        return 0\n",
        "\n",
        "    return dot_product / (magnitude1 * magnitude2)"
      ],
      "metadata": {
        "id": "TQHXPPEtF5Ry"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 手動計算\n",
        "tfidf_vectors_manual, all_words = calculate_tfidf_manual(documents)\n",
        "\n",
        "print(f\"\\n詞彙表 ({len(all_words)} 個詞):\")\n",
        "print(all_words[:10], \"...\")  # 只顯示前10個詞\n",
        "\n",
        "print(\"\\n各文件的 TF-IDF 向量 (部分顯示):\")\n",
        "for i, vec in enumerate(tfidf_vectors_manual):\n",
        "    print(f\"\\n文件 {i+1}: {documents[i][:30]}...\")\n",
        "    # 只顯示非零的 TF-IDF 值\n",
        "    non_zero = {k: f\"{v:.4f}\" for k, v in vec.items() if v > 0}\n",
        "    print(f\"  非零 TF-IDF 值數量: {len(non_zero)}\")\n",
        "    print(f\"  前3個值: {dict(list(non_zero.items())[:3])}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"文件相似度矩陣 (手動計算):\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 計算所有文件對的相似度\n",
        "similarity_matrix_manual = []\n",
        "for i in range(len(documents)):\n",
        "    row = []\n",
        "    for j in range(len(documents)):\n",
        "        sim = cosine_similarity_manual(tfidf_vectors_manual[i],\n",
        "                                      tfidf_vectors_manual[j],\n",
        "                                      all_words)\n",
        "        row.append(sim)\n",
        "    similarity_matrix_manual.append(row)\n",
        "\n",
        "# 顯示相似度矩陣\n",
        "print(\"\\n     \", end=\"\")\n",
        "for i in range(len(documents)):\n",
        "    print(f\"文件{i+1:2d}  \", end=\"\")\n",
        "print()\n",
        "\n",
        "for i, row in enumerate(similarity_matrix_manual):\n",
        "    print(f\"文件{i+1:2d} \", end=\"\")\n",
        "    for val in row:\n",
        "        print(f\"{val:7.4f} \", end=\"\")\n",
        "    print()\n",
        "\n",
        "print(\"\\n文件對相似度分析:\")\n",
        "for i in range(len(documents)):\n",
        "    for j in range(i+1, len(documents)):\n",
        "        sim = similarity_matrix_manual[i][j]\n",
        "        print(f\"文件{i+1} vs 文件{j+1}: {sim:.4f}\")\n",
        "        print(f\"  文件{i+1}: {documents[i][:40]}...\")\n",
        "        print(f\"  文件{j+1}: {documents[j][:40]}...\")\n",
        "        print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbmFfMphGiRK",
        "outputId": "d1ab8ed3-bebd-4f6f-c401-e0160244854b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "詞彙表 (5 個詞):\n",
            "['人工智慧正在改變世界，機器學習是其核心技術', '今天天氣很好，適合出去運動', '機器學習和深度學習都是人工智慧的重要分支', '深度學習推動了人工智慧的發展，特別是在圖像識別領域', '運動有益健康，每天都應該保持運動習慣'] ...\n",
            "\n",
            "各文件的 TF-IDF 向量 (部分顯示):\n",
            "\n",
            "文件 1: 人工智慧正在改變世界，機器學習是其核心技術...\n",
            "  非零 TF-IDF 值數量: 1\n",
            "  前3個值: {'人工智慧正在改變世界，機器學習是其核心技術': '1.6094'}\n",
            "\n",
            "文件 2: 深度學習推動了人工智慧的發展，特別是在圖像識別領域...\n",
            "  非零 TF-IDF 值數量: 1\n",
            "  前3個值: {'深度學習推動了人工智慧的發展，特別是在圖像識別領域': '1.6094'}\n",
            "\n",
            "文件 3: 今天天氣很好，適合出去運動...\n",
            "  非零 TF-IDF 值數量: 1\n",
            "  前3個值: {'今天天氣很好，適合出去運動': '1.6094'}\n",
            "\n",
            "文件 4: 機器學習和深度學習都是人工智慧的重要分支...\n",
            "  非零 TF-IDF 值數量: 1\n",
            "  前3個值: {'機器學習和深度學習都是人工智慧的重要分支': '1.6094'}\n",
            "\n",
            "文件 5: 運動有益健康，每天都應該保持運動習慣...\n",
            "  非零 TF-IDF 值數量: 1\n",
            "  前3個值: {'運動有益健康，每天都應該保持運動習慣': '1.6094'}\n",
            "\n",
            "================================================================================\n",
            "文件相似度矩陣 (手動計算):\n",
            "================================================================================\n",
            "\n",
            "     文件 1  文件 2  文件 3  文件 4  文件 5  \n",
            "文件 1  1.0000  0.0000  0.0000  0.0000  0.0000 \n",
            "文件 2  0.0000  1.0000  0.0000  0.0000  0.0000 \n",
            "文件 3  0.0000  0.0000  1.0000  0.0000  0.0000 \n",
            "文件 4  0.0000  0.0000  0.0000  1.0000  0.0000 \n",
            "文件 5  0.0000  0.0000  0.0000  0.0000  1.0000 \n",
            "\n",
            "文件對相似度分析:\n",
            "文件1 vs 文件2: 0.0000\n",
            "  文件1: 人工智慧正在改變世界，機器學習是其核心技術...\n",
            "  文件2: 深度學習推動了人工智慧的發展，特別是在圖像識別領域...\n",
            "\n",
            "文件1 vs 文件3: 0.0000\n",
            "  文件1: 人工智慧正在改變世界，機器學習是其核心技術...\n",
            "  文件3: 今天天氣很好，適合出去運動...\n",
            "\n",
            "文件1 vs 文件4: 0.0000\n",
            "  文件1: 人工智慧正在改變世界，機器學習是其核心技術...\n",
            "  文件4: 機器學習和深度學習都是人工智慧的重要分支...\n",
            "\n",
            "文件1 vs 文件5: 0.0000\n",
            "  文件1: 人工智慧正在改變世界，機器學習是其核心技術...\n",
            "  文件5: 運動有益健康，每天都應該保持運動習慣...\n",
            "\n",
            "文件2 vs 文件3: 0.0000\n",
            "  文件2: 深度學習推動了人工智慧的發展，特別是在圖像識別領域...\n",
            "  文件3: 今天天氣很好，適合出去運動...\n",
            "\n",
            "文件2 vs 文件4: 0.0000\n",
            "  文件2: 深度學習推動了人工智慧的發展，特別是在圖像識別領域...\n",
            "  文件4: 機器學習和深度學習都是人工智慧的重要分支...\n",
            "\n",
            "文件2 vs 文件5: 0.0000\n",
            "  文件2: 深度學習推動了人工智慧的發展，特別是在圖像識別領域...\n",
            "  文件5: 運動有益健康，每天都應該保持運動習慣...\n",
            "\n",
            "文件3 vs 文件4: 0.0000\n",
            "  文件3: 今天天氣很好，適合出去運動...\n",
            "  文件4: 機器學習和深度學習都是人工智慧的重要分支...\n",
            "\n",
            "文件3 vs 文件5: 0.0000\n",
            "  文件3: 今天天氣很好，適合出去運動...\n",
            "  文件5: 運動有益健康，每天都應該保持運動習慣...\n",
            "\n",
            "文件4 vs 文件5: 0.0000\n",
            "  文件4: 機器學習和深度學習都是人工智慧的重要分支...\n",
            "  文件5: 運動有益健康，每天都應該保持運動習慣...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 第二部分：使用 scikit-learn 實作 ====================\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "def calculate_tfidf_sklearn(documents):\n",
        "    \"\"\"\n",
        "    使用 scikit-learn 計算 TF-IDF 和相似度\n",
        "    \"\"\"\n",
        "    # 建立 TF-IDF 向量化器\n",
        "    vectorizer = TfidfVectorizer()\n",
        "\n",
        "    # 計算 TF-IDF 矩陣\n",
        "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "    # 計算餘弦相似度矩陣\n",
        "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "    return tfidf_matrix, similarity_matrix, vectorizer"
      ],
      "metadata": {
        "id": "83lN_Sn-G08g"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用 scikit-learn 計算\n",
        "tfidf_matrix, similarity_matrix_sklearn, vectorizer = calculate_tfidf_sklearn(documents)\n",
        "\n",
        "print(f\"\\nTF-IDF 矩陣形狀: {tfidf_matrix.shape}\")\n",
        "print(f\"詞彙表大小: {len(vectorizer.get_feature_names_out())}\")\n",
        "print(f\"詞彙表 (前10個): {vectorizer.get_feature_names_out()[:10]}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"文件相似度矩陣 (scikit-learn):\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n     \", end=\"\")\n",
        "for i in range(len(documents)):\n",
        "    print(f\"文件{i+1:2d}  \", end=\"\")\n",
        "print()\n",
        "\n",
        "for i, row in enumerate(similarity_matrix_sklearn):\n",
        "    print(f\"文件{i+1:2d} \", end=\"\")\n",
        "    for val in row:\n",
        "        print(f\"{val:7.4f} \", end=\"\")\n",
        "    print()\n",
        "\n",
        "print(\"\\n文件對相似度分析:\")\n",
        "for i in range(len(documents)):\n",
        "    for j in range(i+1, len(documents)):\n",
        "        sim = similarity_matrix_sklearn[i][j]\n",
        "        print(f\"文件{i+1} vs 文件{j+1}: {sim:.4f}\")\n",
        "        print(f\"  文件{i+1}: {documents[i][:40]}...\")\n",
        "        print(f\"  文件{j+1}: {documents[j][:40]}...\")\n",
        "        print()\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"結論\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n相似度最高的文件對:\")\n",
        "max_sim = 0\n",
        "max_pair = (0, 0)\n",
        "for i in range(len(documents)):\n",
        "    for j in range(i+1, len(documents)):\n",
        "        if similarity_matrix_sklearn[i][j] > max_sim:\n",
        "            max_sim = similarity_matrix_sklearn[i][j]\n",
        "            max_pair = (i, j)\n",
        "\n",
        "i, j = max_pair\n",
        "print(f\"文件{i+1} 和 文件{j+1}, 相似度: {max_sim:.4f}\")\n",
        "print(f\"  文件{i+1}: {documents[i]}\")\n",
        "print(f\"  文件{j+1}: {documents[j]}\")\n",
        "\n",
        "print(\"\\n相似度最低的文件對:\")\n",
        "min_sim = 1\n",
        "min_pair = (0, 0)\n",
        "for i in range(len(documents)):\n",
        "    for j in range(i+1, len(documents)):\n",
        "        if similarity_matrix_sklearn[i][j] < min_sim:\n",
        "            min_sim = similarity_matrix_sklearn[i][j]\n",
        "            min_pair = (i, j)\n",
        "\n",
        "i, j = min_pair\n",
        "print(f\"文件{i+1} 和 文件{j+1}, 相似度: {min_sim:.4f}\")\n",
        "print(f\"  文件{i+1}: {documents[i]}\")\n",
        "print(f\"  文件{j+1}: {documents[j]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fcs3k26wG8v6",
        "outputId": "6a60ebae-0138-4bcb-fbee-09a799609e2a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF 矩陣形狀: (5, 9)\n",
            "詞彙表大小: 9\n",
            "詞彙表 (前10個): ['人工智慧正在改變世界' '今天天氣很好' '機器學習和深度學習都是人工智慧的重要分支' '機器學習是其核心技術' '每天都應該保持運動習慣'\n",
            " '深度學習推動了人工智慧的發展' '特別是在圖像識別領域' '運動有益健康' '適合出去運動']\n",
            "\n",
            "================================================================================\n",
            "文件相似度矩陣 (scikit-learn):\n",
            "================================================================================\n",
            "\n",
            "     文件 1  文件 2  文件 3  文件 4  文件 5  \n",
            "文件 1  1.0000  0.0000  0.0000  0.0000  0.0000 \n",
            "文件 2  0.0000  1.0000  0.0000  0.0000  0.0000 \n",
            "文件 3  0.0000  0.0000  1.0000  0.0000  0.0000 \n",
            "文件 4  0.0000  0.0000  0.0000  1.0000  0.0000 \n",
            "文件 5  0.0000  0.0000  0.0000  0.0000  1.0000 \n",
            "\n",
            "文件對相似度分析:\n",
            "文件1 vs 文件2: 0.0000\n",
            "  文件1: 人工智慧正在改變世界，機器學習是其核心技術...\n",
            "  文件2: 深度學習推動了人工智慧的發展，特別是在圖像識別領域...\n",
            "\n",
            "文件1 vs 文件3: 0.0000\n",
            "  文件1: 人工智慧正在改變世界，機器學習是其核心技術...\n",
            "  文件3: 今天天氣很好，適合出去運動...\n",
            "\n",
            "文件1 vs 文件4: 0.0000\n",
            "  文件1: 人工智慧正在改變世界，機器學習是其核心技術...\n",
            "  文件4: 機器學習和深度學習都是人工智慧的重要分支...\n",
            "\n",
            "文件1 vs 文件5: 0.0000\n",
            "  文件1: 人工智慧正在改變世界，機器學習是其核心技術...\n",
            "  文件5: 運動有益健康，每天都應該保持運動習慣...\n",
            "\n",
            "文件2 vs 文件3: 0.0000\n",
            "  文件2: 深度學習推動了人工智慧的發展，特別是在圖像識別領域...\n",
            "  文件3: 今天天氣很好，適合出去運動...\n",
            "\n",
            "文件2 vs 文件4: 0.0000\n",
            "  文件2: 深度學習推動了人工智慧的發展，特別是在圖像識別領域...\n",
            "  文件4: 機器學習和深度學習都是人工智慧的重要分支...\n",
            "\n",
            "文件2 vs 文件5: 0.0000\n",
            "  文件2: 深度學習推動了人工智慧的發展，特別是在圖像識別領域...\n",
            "  文件5: 運動有益健康，每天都應該保持運動習慣...\n",
            "\n",
            "文件3 vs 文件4: 0.0000\n",
            "  文件3: 今天天氣很好，適合出去運動...\n",
            "  文件4: 機器學習和深度學習都是人工智慧的重要分支...\n",
            "\n",
            "文件3 vs 文件5: 0.0000\n",
            "  文件3: 今天天氣很好，適合出去運動...\n",
            "  文件5: 運動有益健康，每天都應該保持運動習慣...\n",
            "\n",
            "文件4 vs 文件5: 0.0000\n",
            "  文件4: 機器學習和深度學習都是人工智慧的重要分支...\n",
            "  文件5: 運動有益健康，每天都應該保持運動習慣...\n",
            "\n",
            "\n",
            "================================================================================\n",
            "結論\n",
            "================================================================================\n",
            "\n",
            "相似度最高的文件對:\n",
            "文件1 和 文件1, 相似度: 0.0000\n",
            "  文件1: 人工智慧正在改變世界，機器學習是其核心技術\n",
            "  文件1: 人工智慧正在改變世界，機器學習是其核心技術\n",
            "\n",
            "相似度最低的文件對:\n",
            "文件1 和 文件2, 相似度: 0.0000\n",
            "  文件1: 人工智慧正在改變世界，機器學習是其核心技術\n",
            "  文件2: 深度學習推動了人工智慧的發展，特別是在圖像識別領域\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#A-2 基於規則的文本分類\n",
        "# ==================== 測試資料 ====================\n",
        "\n",
        "test_texts = [\n",
        "    \"這家餐廳的牛肉麵真的太好吃了，湯頭濃郁，麵條Q彈，下次一定再來！\",\n",
        "    \"最新的AI技術突破讓人驚艷，深度學習模型的表現越來越好\",\n",
        "    \"這部電影劇情空洞，演技糟糕，完全是浪費時間\",\n",
        "    \"每天慢跑5公里，配合適當的重訓，體能進步很多\"\n",
        "]"
      ],
      "metadata": {
        "id": "pdX96rajJ4dv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 1. 情感分類器  ====================\n",
        "\n",
        "class RuleBasedSentimentClassifier:\n",
        "    def __init__(self):\n",
        "        # 建立正負面詞彙庫\n",
        "        self.positive_words = ['好', '棒', '優秀', '喜歡', '推薦',\n",
        "                               '滿意', '開心', '值得', '精彩', '完美']\n",
        "\n",
        "        self.negative_words = ['差', '糟', '失望', '討厭', '不推薦',\n",
        "                               '浪費', '無聊', '爛', '糟糕', '差勁']\n",
        "\n",
        "        # 加入否定詞處理\n",
        "        self.negation_words = ['不', '沒', '無', '非', '別']\n",
        "\n",
        "    def classify(self, text):\n",
        "        \"\"\"\n",
        "        分類邏輯:\n",
        "        1. 計算正負詞數量\n",
        "        2. 處理否定詞 (否定詞+正面詞=負面)\n",
        "        3. 考慮程度副詞的加權\n",
        "        4. 返回: 正面/負面/中性\n",
        "        \"\"\"\n",
        "        positive_count = 0\n",
        "        negative_count = 0\n",
        "\n",
        "        # 程度副詞加權\n",
        "        degree_words = {\n",
        "            '很': 1.5,\n",
        "            '非常': 2.0,\n",
        "            '超': 2.0,\n",
        "            '太': 1.8,\n",
        "            '特別': 1.5,\n",
        "            '真的': 1.3,\n",
        "            '真': 1.3,\n",
        "            '十分': 1.8,\n",
        "            '相當': 1.5\n",
        "        }\n",
        "\n",
        "        # 將文本轉換為字符列表進行分析\n",
        "        words = list(text)\n",
        "\n",
        "        # 遍歷文本\n",
        "        i = 0\n",
        "        while i < len(text):\n",
        "            # 檢查是否有程度副詞\n",
        "            degree_multiplier = 1.0\n",
        "            for degree_word, weight in degree_words.items():\n",
        "                if text[i:i+len(degree_word)] == degree_word:\n",
        "                    degree_multiplier = weight\n",
        "                    i += len(degree_word)\n",
        "                    break\n",
        "\n",
        "            # 檢查是否有否定詞\n",
        "            is_negated = False\n",
        "            for neg_word in self.negation_words:\n",
        "                if text[i:i+len(neg_word)] == neg_word:\n",
        "                    is_negated = True\n",
        "                    i += len(neg_word)\n",
        "                    break\n",
        "\n",
        "            # 檢查正面詞\n",
        "            found = False\n",
        "            for pos_word in self.positive_words:\n",
        "                if text[i:i+len(pos_word)] == pos_word:\n",
        "                    if is_negated:\n",
        "                        # 否定詞 + 正面詞 = 負面\n",
        "                        negative_count += degree_multiplier\n",
        "                    else:\n",
        "                        positive_count += degree_multiplier\n",
        "                    i += len(pos_word)\n",
        "                    found = True\n",
        "                    break\n",
        "\n",
        "            if found:\n",
        "                continue\n",
        "\n",
        "            # 檢查負面詞\n",
        "            for neg_word in self.negative_words:\n",
        "                if text[i:i+len(neg_word)] == neg_word:\n",
        "                    if is_negated:\n",
        "                        # 否定詞 + 負面詞 = 正面\n",
        "                        positive_count += degree_multiplier\n",
        "                    else:\n",
        "                        negative_count += degree_multiplier\n",
        "                    i += len(neg_word)\n",
        "                    found = True\n",
        "                    break\n",
        "\n",
        "            if not found:\n",
        "                i += 1\n",
        "\n",
        "        # 判斷情感\n",
        "        if positive_count > negative_count:\n",
        "            sentiment = '正面'\n",
        "            score = positive_count - negative_count\n",
        "        elif negative_count > positive_count:\n",
        "            sentiment = '負面'\n",
        "            score = negative_count - positive_count\n",
        "        else:\n",
        "            sentiment = '中性'\n",
        "            score = 0\n",
        "\n",
        "        return {\n",
        "            'sentiment': sentiment,\n",
        "            'positive_score': positive_count,\n",
        "            'negative_score': negative_count,\n",
        "            'confidence': score\n",
        "        }\n",
        "\n",
        "\n",
        "# ==================== 情感分類器測試 ====================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"1. 情感分類器測試\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "sentiment_classifier = RuleBasedSentimentClassifier()\n",
        "\n",
        "for i, text in enumerate(test_texts, 1):\n",
        "    print(f\"\\n測試文本 {i}: {text}\")\n",
        "    result = sentiment_classifier.classify(text)\n",
        "    print(f\"  情感: {result['sentiment']}\")\n",
        "    print(f\"  正面分數: {result['positive_score']:.2f}\")\n",
        "    print(f\"  負面分數: {result['negative_score']:.2f}\")\n",
        "    print(f\"  信心度: {result['confidence']:.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"情感分類統計\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "sentiment_stats = {'正面': 0, '負面': 0, '中性': 0}\n",
        "for text in test_texts:\n",
        "    result = sentiment_classifier.classify(text)\n",
        "    sentiment_stats[result['sentiment']] += 1\n",
        "\n",
        "print(f\"正面文本數量: {sentiment_stats['正面']}\")\n",
        "print(f\"負面文本數量: {sentiment_stats['負面']}\")\n",
        "print(f\"中性文本數量: {sentiment_stats['中性']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SugXNoVHLILS",
        "outputId": "16c868da-6488-4b1e-9ba4-4ff546929c64"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "1. 情感分類器測試\n",
            "================================================================================\n",
            "\n",
            "測試文本 1: 這家餐廳的牛肉麵真的太好吃了，湯頭濃郁，麵條Q彈，下次一定再來！\n",
            "  情感: 正面\n",
            "  正面分數: 1.00\n",
            "  負面分數: 0.00\n",
            "  信心度: 1.00\n",
            "\n",
            "測試文本 2: 最新的AI技術突破讓人驚艷，深度學習模型的表現越來越好\n",
            "  情感: 正面\n",
            "  正面分數: 1.00\n",
            "  負面分數: 0.00\n",
            "  信心度: 1.00\n",
            "\n",
            "測試文本 3: 這部電影劇情空洞，演技糟糕，完全是浪費時間\n",
            "  情感: 負面\n",
            "  正面分數: 0.00\n",
            "  負面分數: 2.00\n",
            "  信心度: 2.00\n",
            "\n",
            "測試文本 4: 每天慢跑5公里，配合適當的重訓，體能進步很多\n",
            "  情感: 中性\n",
            "  正面分數: 0.00\n",
            "  負面分數: 0.00\n",
            "  信心度: 0.00\n",
            "\n",
            "================================================================================\n",
            "情感分類統計\n",
            "================================================================================\n",
            "正面文本數量: 2\n",
            "負面文本數量: 1\n",
            "中性文本數量: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 2. 主題分類器 ====================\n",
        "\n",
        "class TopicClassifier:\n",
        "    def __init__(self):\n",
        "        self.topic_keywords = {\n",
        "            '科技': ['AI', '人工智慧', '電腦', '軟體', '程式', '演算法'],\n",
        "            '運動': ['運動', '健身', '跑步', '游泳', '球類', '比賽'],\n",
        "            '美食': ['吃', '食物', '餐廳', '美味', '料理', '烹飪'],\n",
        "            '旅遊': ['旅行', '景點', '飯店', '機票', '觀光', '度假']\n",
        "        }\n",
        "\n",
        "    def classify(self, text):\n",
        "        \"\"\"返回最可能的主題\"\"\"\n",
        "        topic_scores = {topic: 0 for topic in self.topic_keywords.keys()}\n",
        "\n",
        "        # 計算每個主題的關鍵詞匹配數\n",
        "        for topic, keywords in self.topic_keywords.items():\n",
        "            for keyword in keywords:\n",
        "                # 計算關鍵詞在文本中出現的次數\n",
        "                count = text.count(keyword)\n",
        "                topic_scores[topic] += count\n",
        "\n",
        "        # 找出得分最高的主題\n",
        "        max_score = max(topic_scores.values())\n",
        "\n",
        "        if max_score == 0:\n",
        "            return {\n",
        "                'topic': '未分類',\n",
        "                'confidence': 0,\n",
        "                'scores': topic_scores\n",
        "            }\n",
        "\n",
        "        best_topic = max(topic_scores, key=topic_scores.get)\n",
        "\n",
        "        return {\n",
        "            'topic': best_topic,\n",
        "            'confidence': max_score,\n",
        "            'scores': topic_scores\n",
        "        }\n",
        "\n",
        "\n",
        "# ==================== 主題分類器測試 ====================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"2. 主題分類器測試\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "topic_classifier = TopicClassifier()\n",
        "\n",
        "for i, text in enumerate(test_texts, 1):\n",
        "    print(f\"\\n測試文本 {i}: {text}\")\n",
        "    result = topic_classifier.classify(text)\n",
        "    print(f\"  主題: {result['topic']}\")\n",
        "    print(f\"  信心度: {result['confidence']}\")\n",
        "    print(f\"  各主題得分: {result['scores']}\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"主題分類統計\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "topic_stats = {}\n",
        "for text in test_texts:\n",
        "    result = topic_classifier.classify(text)\n",
        "    topic = result['topic']\n",
        "    if topic not in topic_stats:\n",
        "        topic_stats[topic] = 0\n",
        "    topic_stats[topic] += 1\n",
        "\n",
        "for topic, count in topic_stats.items():\n",
        "    print(f\"{topic}: {count} 篇\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_l4xbOtLgCs",
        "outputId": "85c752d3-e4a9-4eb2-e79c-2ab5e2c36e92"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "2. 主題分類器測試\n",
            "================================================================================\n",
            "\n",
            "測試文本 1: 這家餐廳的牛肉麵真的太好吃了，湯頭濃郁，麵條Q彈，下次一定再來！\n",
            "  主題: 美食\n",
            "  信心度: 2\n",
            "  各主題得分: {'科技': 0, '運動': 0, '美食': 2, '旅遊': 0}\n",
            "\n",
            "測試文本 2: 最新的AI技術突破讓人驚艷，深度學習模型的表現越來越好\n",
            "  主題: 科技\n",
            "  信心度: 1\n",
            "  各主題得分: {'科技': 1, '運動': 0, '美食': 0, '旅遊': 0}\n",
            "\n",
            "測試文本 3: 這部電影劇情空洞，演技糟糕，完全是浪費時間\n",
            "  主題: 未分類\n",
            "  信心度: 0\n",
            "  各主題得分: {'科技': 0, '運動': 0, '美食': 0, '旅遊': 0}\n",
            "\n",
            "測試文本 4: 每天慢跑5公里，配合適當的重訓，體能進步很多\n",
            "  主題: 未分類\n",
            "  信心度: 0\n",
            "  各主題得分: {'科技': 0, '運動': 0, '美食': 0, '旅遊': 0}\n",
            "\n",
            "================================================================================\n",
            "主題分類統計\n",
            "================================================================================\n",
            "美食: 1 篇\n",
            "科技: 1 篇\n",
            "未分類: 2 篇\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# A-3 統計式自動摘要\n",
        "\n",
        "class StatisticalSummarizer:\n",
        "    def __init__(self):\n",
        "        # 定義停用詞 (過濾掉不具備實質意義的常用字)\n",
        "        self.stop_words = set(['的', '了', '在', '是', '我', '有', '和',\n",
        "                             '就', '不', '人', '都', '一', '一個', '上',\n",
        "                             '也', '很', '到', '說', '要', '去', '你',\n",
        "                             '這', '那', '會', '為', '著', '之', '與',\n",
        "                             '及', '其', '或', '但', '而'])\n",
        "\n",
        "    def sentence_score(self, sentence, word_freq, index=0, total_sentences=1):\n",
        "        \"\"\"\n",
        "        計算句子重要性分數\n",
        "        Args:\n",
        "            sentence: 句子字串\n",
        "            word_freq: 字詞頻率字典 (正規化後的)\n",
        "            index: 句子在文章中的位置 (用於位置加權)\n",
        "            total_sentences: 總句數\n",
        "        \"\"\"\n",
        "        if not sentence:\n",
        "            return 0\n",
        "\n",
        "        score = 0\n",
        "        # 簡單分詞：這裡以「字元」為單位，過濾停用詞\n",
        "        words = [w for w in sentence if w not in self.stop_words]\n",
        "\n",
        "        # 1. 關鍵詞頻率得分 (累加該句包含的詞之權重)\n",
        "        for w in words:\n",
        "            score += word_freq.get(w, 0)\n",
        "\n",
        "        # 2. 位置加權 (Position Weighting)\n",
        "        # 首句通常包含主旨，給予較高權重\n",
        "        if index == 0:\n",
        "            score *= 2.0\n",
        "        # 尾句通常是總結，給予次高權重\n",
        "        elif index == total_sentences - 1:\n",
        "            score *= 1.5\n",
        "\n",
        "        # 3. 長度懲罰 (可選)\n",
        "        # 避免選到過短的句子\n",
        "        if len(sentence) < 5:\n",
        "            score *= 0.5\n",
        "\n",
        "        return score\n",
        "\n",
        "    def summarize(self, text, ratio=0.3):\n",
        "        \"\"\"\n",
        "        摘要生成主程序\n",
        "        Args:\n",
        "            text: 原文\n",
        "            ratio: 摘要比例 (例如 0.3 代表保留 30% 的句子)\n",
        "        \"\"\"\n",
        "        # 1. 分句 (處理中文標點 。！？)\n",
        "        # 使用正規表達式分割，並保留標點符號\n",
        "        sentences = re.split(r'([。！？])', text)\n",
        "\n",
        "        # 將標點符號接回句子後面\n",
        "        clean_sentences = []\n",
        "        for i in range(0, len(sentences)-1, 2):\n",
        "            if sentences[i].strip():\n",
        "                clean_sentences.append(sentences[i] + sentences[i+1])\n",
        "\n",
        "        # 處理最後可能剩餘的部分\n",
        "        if len(sentences) % 2 != 0 and sentences[-1].strip():\n",
        "             clean_sentences.append(sentences[-1])\n",
        "\n",
        "        if not clean_sentences:\n",
        "            return \"\", []\n",
        "\n",
        "        # 2. 計算全文字頻 (作為重要性依據)\n",
        "        all_content = \"\".join(clean_sentences)\n",
        "        all_words = [c for c in all_content if c not in self.stop_words and c.strip() and c not in '。！？，、：；「」『』']\n",
        "        word_counts = Counter(all_words)\n",
        "\n",
        "        # 正規化詞頻 (TF)\n",
        "        total_words = len(all_words)\n",
        "        word_freq_norm = {k: v/total_words for k, v in word_counts.items()}\n",
        "\n",
        "        # 3. 計算每個句子的重要性分數\n",
        "        sentence_scores = []\n",
        "        for i, sent in enumerate(clean_sentences):\n",
        "            score = self.sentence_score(sent, word_freq_norm, index=i, total_sentences=len(clean_sentences))\n",
        "            sentence_scores.append((i, sent, score))\n",
        "\n",
        "        # 4. 選擇最高分的句子\n",
        "        num_sentences = max(1, int(len(clean_sentences) * ratio))\n",
        "\n",
        "        # 依分數排序 (由高到低) 並選取前 N 句\n",
        "        top_sentences = sorted(sentence_scores, key=lambda x: x[2], reverse=True)[:num_sentences]\n",
        "\n",
        "        # 5. 按原文順序重新排列 (依 index 排序)\n",
        "        top_sentences.sort(key=lambda x: x[0])\n",
        "\n",
        "        # 組合結果\n",
        "        summary = \"\".join([item[1] for item in top_sentences])\n",
        "\n",
        "        return summary, sentence_scores\n",
        "\n",
        "# ==================== 測試資料與執行 ====================\n",
        "\n",
        "# 測試文章：關於人工智慧的短文\n",
        "article = \"\"\"人工智慧 (AI) 的發展正在深刻改變我們的生活方式。從早上起床時的智慧鬧鐘，到通勤時的路線規劃，再到工作中的各種輔助工具，AI無處不在。\n",
        "\n",
        "在醫療領域，AI協助醫生進行疾病診斷，提高了診斷的準確率和效率。透過分析大量的醫療影像和病歷資料，AI能夠發現人眼容易忽略的細節，為患者提供更好的治療方案。\n",
        "\n",
        "教育方面，AI個人化學習系統能夠根據每個學生的學習進度和特點，提供客製化的教學內容。這種因材施教的方式，讓學習變得更加高效和有趣。\n",
        "\n",
        "然而，AI的快速發展也帶來了一些挑戰。首先是就業問題，許多傳統工作可能會被AI取代。其次是隱私和安全問題，AI系統需要大量數據來訓練，如何保護個人隱私成為重要議題。最後是倫理問題，AI的決策過程往往缺乏透明度，可能會產生偏見或歧視。\n",
        "\n",
        "面對這些挑戰，我們需要在推動AI發展的同時，建立相應的法律法規和倫理準則。只有這樣，才能確保AI技術真正為人類福祉服務，創造一個更美好的未來。\"\"\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"3. 統計式自動摘要測試\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "summarizer = StatisticalSummarizer()\n",
        "summary, scores = summarizer.summarize(article, ratio=0.3)\n",
        "\n",
        "print(f\"\\n原文長度: {len(article)} 字\")\n",
        "print(f\"原文句數: {len(scores)} 句\")\n",
        "print(f\"預計選取句數: {max(1, int(len(scores) * 0.3))} 句\")\n",
        "print(\"-\" * 40)\n",
        "print(\"各句子得分分析 (前5名):\")\n",
        "\n",
        "# 顯示分數最高的前5個句子\n",
        "sorted_scores = sorted(scores, key=lambda x: x[2], reverse=True)\n",
        "for i, (idx, sent, score) in enumerate(sorted_scores[:5], 1):\n",
        "    print(f\"Rank {i} (Index {idx}): {sent[:20]}... [Score: {score:.4f}]\")\n",
        "\n",
        "print(\"-\" * 40)\n",
        "print(f\"生成摘要結果:\\n\")\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKChIf7kXfCM",
        "outputId": "31f5389c-d68a-4b51-f9ac-0d9591a1df86"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "3. 統計式自動摘要測試\n",
            "================================================================================\n",
            "\n",
            "原文長度: 407 字\n",
            "原文句數: 12 句\n",
            "預計選取句數: 3 句\n",
            "----------------------------------------\n",
            "各句子得分分析 (前5名):\n",
            "Rank 1 (Index 0): 人工智慧 (AI) 的發展正在深刻改變我... [Score: 0.3907]\n",
            "Rank 2 (Index 4): \n",
            "\n",
            "教育方面，AI個人化學習系統能夠根據... [Score: 0.3609]\n",
            "Rank 3 (Index 11): 只有這樣，才能確保AI技術真正為人類福祉... [Score: 0.2980]\n",
            "Rank 4 (Index 3): 透過分析大量的醫療影像和病歷資料，AI能... [Score: 0.2848]\n",
            "Rank 5 (Index 8): 其次是隱私和安全問題，AI系統需要大量數... [Score: 0.2483]\n",
            "----------------------------------------\n",
            "生成摘要結果:\n",
            "\n",
            "人工智慧 (AI) 的發展正在深刻改變我們的生活方式。\n",
            "\n",
            "教育方面，AI個人化學習系統能夠根據每個學生的學習進度和特點，提供客製化的教學內容。只有這樣，才能確保AI技術真正為人類福祉服務，創造一個更美好的未來。\n"
          ]
        }
      ]
    }
  ]
}